<html><body>
<style>

body, h1, h2, h3, div, span, p, pre, a {
  margin: 0;
  padding: 0;
  border: 0;
  font-weight: inherit;
  font-style: inherit;
  font-size: 100%;
  font-family: inherit;
  vertical-align: baseline;
}

body {
  font-size: 13px;
  padding: 1em;
}

h1 {
  font-size: 26px;
  margin-bottom: 1em;
}

h2 {
  font-size: 24px;
  margin-bottom: 1em;
}

h3 {
  font-size: 20px;
  margin-bottom: 1em;
  margin-top: 1em;
}

pre, code {
  line-height: 1.5;
  font-family: Monaco, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console', monospace;
}

pre {
  margin-top: 0.5em;
}

h1, h2, h3, p {
  font-family: Arial, sans serif;
}

h1, h2, h3 {
  border-bottom: solid #CCC 1px;
}

.toc_element {
  margin-top: 0.5em;
}

.firstline {
  margin-left: 2 em;
}

.method  {
  margin-top: 1em;
  border: solid 1px #CCC;
  padding: 1em;
  background: #EEE;
}

.details {
  font-weight: bold;
  font-size: 14px;
}

</style>

<h1><a href="cloudasset_v1p7beta1.html">Cloud Asset API</a> . <a href="cloudasset_v1p7beta1.projects.html">projects</a> . <a href="cloudasset_v1p7beta1.projects.inventorySettings.html">inventorySettings</a> . <a href="cloudasset_v1p7beta1.projects.inventorySettings.exportSettings.html">exportSettings</a></h1>
<h2>Instance Methods</h2>
<p class="toc_element">
  <code><a href="#close">close()</a></code></p>
<p class="firstline">Close httplib2 connections.</p>
<p class="toc_element">
  <code><a href="#create">create(parent, body=None, x__xgafv=None)</a></code></p>
<p class="firstline">Creates an export setting.</p>
<p class="toc_element">
  <code><a href="#delete">delete(name, x__xgafv=None)</a></code></p>
<p class="firstline">Deletes an export setting.</p>
<p class="toc_element">
  <code><a href="#get">get(name, x__xgafv=None)</a></code></p>
<p class="firstline">Gets details about an export setting.</p>
<p class="toc_element">
  <code><a href="#list">list(parent, pageSize=None, pageToken=None, x__xgafv=None)</a></code></p>
<p class="firstline">Lists all export settings under a [InventorySettings].</p>
<p class="toc_element">
  <code><a href="#list_next">list_next(previous_request, previous_response)</a></code></p>
<p class="firstline">Retrieves the next page of results.</p>
<p class="toc_element">
  <code><a href="#patch">patch(name, body=None, updateMask=None, x__xgafv=None)</a></code></p>
<p class="firstline">Updates an export setting.</p>
<h3>Method Details</h3>
<div class="method">
    <code class="details" id="close">close()</code>
  <pre>Close httplib2 connections.</pre>
</div>

<div class="method">
    <code class="details" id="create">create(parent, body=None, x__xgafv=None)</code>
  <pre>Creates an export setting.

Args:
  parent: string, Required. Name of the inventory settings where this export setting should be created in. The format will be: projects/{PROJECT_NUMBER}/inventorySettings folders/{FOLDER_NUMBER}/inventorySettings organizations/{ORGANIZATION_NUMBER}/inventorySettings Currently a maximum of 100 export setting can be created under each [InventorySettings]. (required)
  body: object, The request body.
    The object takes the form of:

{
  &quot;exportSetting&quot;: { # Settings that configure CAI to export assets on given schedules. An [InventorySettings] might have multiple [ExportSetting], each with a unique identifier. # Required. The export setting details. The field `name` must be empty and it will be generated in the format of: projects/{PROJECT_NUMBER}/inventorySettings/exportSettings/{EXPORT_SETTING_ID} folders/{FOLDER_NUMBER}/inventorySettings/exportSettings/{EXPORT_SETTING_ID} organizations/{ORGANIZATION_NUMBER}/inventorySettings/exportSettings/{EXPORT_SETTING_ID}
    &quot;assetTypes&quot;: [ # Asset types of resources that will be exported on schedule. For example: &quot;compute.googleapis.com/Disk&quot;. Regular expressions are also supported. For example: * &quot;compute.googleapis.com.*&quot; snapshots resources whose asset type starts with &quot;compute.googleapis.com&quot;. * &quot;.*Instance&quot; snapshots resources whose asset type ends with &quot;Instance&quot;. * &quot;.*Instance.*&quot; snapshots resources whose asset type contains &quot;Instance&quot;. See [RE2](https://github.com/google/re2/wiki/Syntax) for all supported regular expression syntax. If the regular expression does not match any supported asset type, an INVALID_ARGUMENT error will be returned. If specified, only matching assets will be returned. See [Introduction to Cloud Asset Inventory](https://cloud.google.com/asset-inventory/docs/overview) for all supported asset types.
      &quot;A String&quot;,
    ],
    &quot;bigquerySetting&quot;: { # Setting for scheduled export to BigQuery. # Setting to configure scheduled export output to BigQuery. At least one of [gcs_settings] and [bigquery_settings] should be set.
      &quot;destination&quot;: { # A BigQuery destination for exporting assets to. # Required. Destination on BigQuery. The output table stores the fields in asset proto as columns in BigQuery.
        &quot;dataset&quot;: &quot;A String&quot;, # Required. The BigQuery dataset in format &quot;projects/projectId/datasets/datasetId&quot;, to which the snapshot result should be exported. If this dataset does not exist, the export call returns an INVALID_ARGUMENT error.
        &quot;force&quot;: True or False, # If the destination table already exists and this flag is `TRUE`, the table will be overwritten by the contents of assets snapshot. If the flag is `FALSE` or unset and the destination table already exists, the export call returns an INVALID_ARGUMEMT error.
        &quot;partitionSpec&quot;: { # Specifications of BigQuery partitioned table as export destination. # [partition_spec] determines whether to export to partitioned table(s) and how to partition the data. If [partition_spec] is unset or [partition_spec.partition_key] is unset or `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to non-partitioned table(s). [force] will decide whether to overwrite existing table(s). If [partition_spec] is specified. First, the snapshot results will be written to partitioned table(s) with two additional timestamp columns, readTime and requestTime, one of which will be the partition key. Secondly, in the case when any destination table already exists, it will first try to update existing table&#x27;s schema as necessary by appending additional columns. Then, if [force] is `TRUE`, the corresponding partition will be overwritten by the snapshot results (data in different partitions will remain intact); if [force] is unset or `FALSE`, it will append the data. An error will be returned if the schema update or data appension fails.
          &quot;partitionKey&quot;: &quot;A String&quot;, # The partition key for BigQuery partitioned table.
        },
        &quot;separateTablesPerAssetType&quot;: True or False, # If this flag is `TRUE`, the snapshot results will be written to one or multiple tables, each of which contains results of one asset type. The [force] and [partition_spec] fields will apply to each of them. Field [table] will be concatenated with &quot;_&quot; and the asset type names (see https://cloud.google.com/asset-inventory/docs/supported-asset-types for supported asset types) to construct per-asset-type table names, in which all non-alphanumeric characters like &quot;.&quot; and &quot;/&quot; will be substituted by &quot;_&quot;. Example: if field [table] is &quot;mytable&quot; and snapshot results contain &quot;storage.googleapis.com/Bucket&quot; assets, the corresponding table name will be &quot;mytable_storage_googleapis_com_Bucket&quot;. If any of these tables does not exist, a new table with the concatenated name will be created. When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of each table will include RECORD-type columns mapped to the nested fields in the Asset.resource.data field of that asset type (up to the 15 nested level BigQuery supports (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The fields in &gt;15 nested levels will be stored in JSON format string as a child column of its parent RECORD column. If error occurs when exporting to any table, the whole export call will return an error but the export results that already succeed will persist. Example: if exporting to table_type_A succeeds when exporting to table_type_B fails during one export call, the results in table_type_A will persist and there will not be partial results persisting in a table.
        &quot;table&quot;: &quot;A String&quot;, # Required. The BigQuery table to which the snapshot result should be written. If this table does not exist, a new table with the given name will be created. In the case of scheduled export, the real destination is [table] appended with a suffix indicating the point-in-time of the snapshot. For example, suppose field [table] is &quot;mytable&quot;. When [separate_tables_per_asset_type] and is `FALSE`, the destination will be &quot;mytable_snapshot_20201122060000&quot;, and when [separate_tables_per_asset_type] and is `TRUE` and the [asset_types] contains &quot;storage.googleapis.com/Bucket&quot;, the destination will be &quot;mytable_snapshot_20201122060000_storage_googleapis_com_Bucket&quot;
      },
      &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
        &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
        &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
      },
    },
    &quot;contentType&quot;: &quot;A String&quot;, # Required. Asset content type. Currently only `RESOURCE`, `IAM_POLICY`, `ORG_POLICY`, `ACCESS_POLICY` are supported.
    &quot;gcsSetting&quot;: { # Setting for scheduled export to GCS. # Setting to configure scheduled export output to GCS. At least one of [gcs_setting] and [bigquery_setting] should be set.
      &quot;destination&quot;: { # A Cloud Storage location. # Required. Destination on Cloud Storage.
        &quot;uri&quot;: &quot;A String&quot;, # The uri of the Cloud Storage object. It&#x27;s the same uri that is used by gsutil. Example: &quot;gs://bucket_name/object_name&quot;. See [Viewing and Editing Object Metadata](https://cloud.google.com/storage/docs/viewing-editing-metadata) for more information. In the case of scheduled export, a path indiciating when the export happens will be added in between the bucket_name and object_name. For example: the real destination for an scheduled snapshot is: gs://bucket_name/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/file
        &quot;uriPrefix&quot;: &quot;A String&quot;, # The uri prefix of all generated Cloud Storage objects. Example: &quot;gs://bucket_name/object_name_prefix&quot;. Each object uri is in format: &quot;gs://bucket_name/object_name_prefix/{ASSET_TYPE}/{SHARD_NUMBER} and only contains assets for that type. starts from 0. Example: &quot;gs://bucket_name/object_name_prefix/compute.googleapis.com/Disk/0&quot; is the first shard of output objects containing all compute.googleapis.com/Disk assets. An INVALID_ARGUMENT error will be returned if file with the same name &quot;gs://bucket_name/object_name_prefix&quot; already exists. In the case of scheduled export, a path indiciating when the export happens will be appended. For example: the real destination for an scheduled snapshot is: gs://bucket_name/object_name_prefix/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/{ASSET_TYPE}/{SHARD_NUMBER}
      },
      &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
        &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
        &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
      },
    },
    &quot;name&quot;: &quot;A String&quot;, # The format will be: organizations/{ORGANIZATION_NUMBER}/inventorySettings/exportSettings/{exportSetting} or folders/{FOLDER_NUMBER}/inventorySettings/exportSettings/{exportSetting} or projects/{PROJECT_NUMBER}/inventorySettings/exportSettings/{exportSetting}
  },
  &quot;exportSettingId&quot;: &quot;A String&quot;, # Required. This is the client-assigned export setting identifier encoded in UTF-8 and it needs to be unique under a specific [InventorySettings].
}

  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # Settings that configure CAI to export assets on given schedules. An [InventorySettings] might have multiple [ExportSetting], each with a unique identifier.
  &quot;assetTypes&quot;: [ # Asset types of resources that will be exported on schedule. For example: &quot;compute.googleapis.com/Disk&quot;. Regular expressions are also supported. For example: * &quot;compute.googleapis.com.*&quot; snapshots resources whose asset type starts with &quot;compute.googleapis.com&quot;. * &quot;.*Instance&quot; snapshots resources whose asset type ends with &quot;Instance&quot;. * &quot;.*Instance.*&quot; snapshots resources whose asset type contains &quot;Instance&quot;. See [RE2](https://github.com/google/re2/wiki/Syntax) for all supported regular expression syntax. If the regular expression does not match any supported asset type, an INVALID_ARGUMENT error will be returned. If specified, only matching assets will be returned. See [Introduction to Cloud Asset Inventory](https://cloud.google.com/asset-inventory/docs/overview) for all supported asset types.
    &quot;A String&quot;,
  ],
  &quot;bigquerySetting&quot;: { # Setting for scheduled export to BigQuery. # Setting to configure scheduled export output to BigQuery. At least one of [gcs_settings] and [bigquery_settings] should be set.
    &quot;destination&quot;: { # A BigQuery destination for exporting assets to. # Required. Destination on BigQuery. The output table stores the fields in asset proto as columns in BigQuery.
      &quot;dataset&quot;: &quot;A String&quot;, # Required. The BigQuery dataset in format &quot;projects/projectId/datasets/datasetId&quot;, to which the snapshot result should be exported. If this dataset does not exist, the export call returns an INVALID_ARGUMENT error.
      &quot;force&quot;: True or False, # If the destination table already exists and this flag is `TRUE`, the table will be overwritten by the contents of assets snapshot. If the flag is `FALSE` or unset and the destination table already exists, the export call returns an INVALID_ARGUMEMT error.
      &quot;partitionSpec&quot;: { # Specifications of BigQuery partitioned table as export destination. # [partition_spec] determines whether to export to partitioned table(s) and how to partition the data. If [partition_spec] is unset or [partition_spec.partition_key] is unset or `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to non-partitioned table(s). [force] will decide whether to overwrite existing table(s). If [partition_spec] is specified. First, the snapshot results will be written to partitioned table(s) with two additional timestamp columns, readTime and requestTime, one of which will be the partition key. Secondly, in the case when any destination table already exists, it will first try to update existing table&#x27;s schema as necessary by appending additional columns. Then, if [force] is `TRUE`, the corresponding partition will be overwritten by the snapshot results (data in different partitions will remain intact); if [force] is unset or `FALSE`, it will append the data. An error will be returned if the schema update or data appension fails.
        &quot;partitionKey&quot;: &quot;A String&quot;, # The partition key for BigQuery partitioned table.
      },
      &quot;separateTablesPerAssetType&quot;: True or False, # If this flag is `TRUE`, the snapshot results will be written to one or multiple tables, each of which contains results of one asset type. The [force] and [partition_spec] fields will apply to each of them. Field [table] will be concatenated with &quot;_&quot; and the asset type names (see https://cloud.google.com/asset-inventory/docs/supported-asset-types for supported asset types) to construct per-asset-type table names, in which all non-alphanumeric characters like &quot;.&quot; and &quot;/&quot; will be substituted by &quot;_&quot;. Example: if field [table] is &quot;mytable&quot; and snapshot results contain &quot;storage.googleapis.com/Bucket&quot; assets, the corresponding table name will be &quot;mytable_storage_googleapis_com_Bucket&quot;. If any of these tables does not exist, a new table with the concatenated name will be created. When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of each table will include RECORD-type columns mapped to the nested fields in the Asset.resource.data field of that asset type (up to the 15 nested level BigQuery supports (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The fields in &gt;15 nested levels will be stored in JSON format string as a child column of its parent RECORD column. If error occurs when exporting to any table, the whole export call will return an error but the export results that already succeed will persist. Example: if exporting to table_type_A succeeds when exporting to table_type_B fails during one export call, the results in table_type_A will persist and there will not be partial results persisting in a table.
      &quot;table&quot;: &quot;A String&quot;, # Required. The BigQuery table to which the snapshot result should be written. If this table does not exist, a new table with the given name will be created. In the case of scheduled export, the real destination is [table] appended with a suffix indicating the point-in-time of the snapshot. For example, suppose field [table] is &quot;mytable&quot;. When [separate_tables_per_asset_type] and is `FALSE`, the destination will be &quot;mytable_snapshot_20201122060000&quot;, and when [separate_tables_per_asset_type] and is `TRUE` and the [asset_types] contains &quot;storage.googleapis.com/Bucket&quot;, the destination will be &quot;mytable_snapshot_20201122060000_storage_googleapis_com_Bucket&quot;
    },
    &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
      &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
      &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
    },
  },
  &quot;contentType&quot;: &quot;A String&quot;, # Required. Asset content type. Currently only `RESOURCE`, `IAM_POLICY`, `ORG_POLICY`, `ACCESS_POLICY` are supported.
  &quot;gcsSetting&quot;: { # Setting for scheduled export to GCS. # Setting to configure scheduled export output to GCS. At least one of [gcs_setting] and [bigquery_setting] should be set.
    &quot;destination&quot;: { # A Cloud Storage location. # Required. Destination on Cloud Storage.
      &quot;uri&quot;: &quot;A String&quot;, # The uri of the Cloud Storage object. It&#x27;s the same uri that is used by gsutil. Example: &quot;gs://bucket_name/object_name&quot;. See [Viewing and Editing Object Metadata](https://cloud.google.com/storage/docs/viewing-editing-metadata) for more information. In the case of scheduled export, a path indiciating when the export happens will be added in between the bucket_name and object_name. For example: the real destination for an scheduled snapshot is: gs://bucket_name/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/file
      &quot;uriPrefix&quot;: &quot;A String&quot;, # The uri prefix of all generated Cloud Storage objects. Example: &quot;gs://bucket_name/object_name_prefix&quot;. Each object uri is in format: &quot;gs://bucket_name/object_name_prefix/{ASSET_TYPE}/{SHARD_NUMBER} and only contains assets for that type. starts from 0. Example: &quot;gs://bucket_name/object_name_prefix/compute.googleapis.com/Disk/0&quot; is the first shard of output objects containing all compute.googleapis.com/Disk assets. An INVALID_ARGUMENT error will be returned if file with the same name &quot;gs://bucket_name/object_name_prefix&quot; already exists. In the case of scheduled export, a path indiciating when the export happens will be appended. For example: the real destination for an scheduled snapshot is: gs://bucket_name/object_name_prefix/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/{ASSET_TYPE}/{SHARD_NUMBER}
    },
    &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
      &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
      &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
    },
  },
  &quot;name&quot;: &quot;A String&quot;, # The format will be: organizations/{ORGANIZATION_NUMBER}/inventorySettings/exportSettings/{exportSetting} or folders/{FOLDER_NUMBER}/inventorySettings/exportSettings/{exportSetting} or projects/{PROJECT_NUMBER}/inventorySettings/exportSettings/{exportSetting}
}</pre>
</div>

<div class="method">
    <code class="details" id="delete">delete(name, x__xgafv=None)</code>
  <pre>Deletes an export setting.

Args:
  name: string, Required. The name of the export setting and it must be in the format of: projects/{PROJECT_NUMBER}/inventorySettings/exportSettings/{EXPORT_SETTING_ID} folders/{FOLDER_NUMBER}/inventorySettings/exportSettings/{EXPORT_SETTING_ID} organizations/{ORGANIZATION_NUMBER}/inventorySettings/exportSettings/{EXPORT_SETTING_ID} (required)
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # A generic empty message that you can re-use to avoid defining duplicated empty messages in your APIs. A typical example is to use it as the request or the response type of an API method. For instance: service Foo { rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty); } The JSON representation for `Empty` is empty JSON object `{}`.
}</pre>
</div>

<div class="method">
    <code class="details" id="get">get(name, x__xgafv=None)</code>
  <pre>Gets details about an export setting.

Args:
  name: string, Required. The name of the [ExportSetting] and it must be in the format of: projects/{PROJECT_NUMBER}/inventorySettings/exportSettings/{EXPORT_SETTING_ID} folders/{FOLDER_NUMBER}/inventorySettings/exportSettings/{EXPORT_SETTING_ID} organizations/{ORGANIZATION_NUMBER}/inventorySettings/exportSettings/{EXPORT_SETTING_ID} (required)
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # Settings that configure CAI to export assets on given schedules. An [InventorySettings] might have multiple [ExportSetting], each with a unique identifier.
  &quot;assetTypes&quot;: [ # Asset types of resources that will be exported on schedule. For example: &quot;compute.googleapis.com/Disk&quot;. Regular expressions are also supported. For example: * &quot;compute.googleapis.com.*&quot; snapshots resources whose asset type starts with &quot;compute.googleapis.com&quot;. * &quot;.*Instance&quot; snapshots resources whose asset type ends with &quot;Instance&quot;. * &quot;.*Instance.*&quot; snapshots resources whose asset type contains &quot;Instance&quot;. See [RE2](https://github.com/google/re2/wiki/Syntax) for all supported regular expression syntax. If the regular expression does not match any supported asset type, an INVALID_ARGUMENT error will be returned. If specified, only matching assets will be returned. See [Introduction to Cloud Asset Inventory](https://cloud.google.com/asset-inventory/docs/overview) for all supported asset types.
    &quot;A String&quot;,
  ],
  &quot;bigquerySetting&quot;: { # Setting for scheduled export to BigQuery. # Setting to configure scheduled export output to BigQuery. At least one of [gcs_settings] and [bigquery_settings] should be set.
    &quot;destination&quot;: { # A BigQuery destination for exporting assets to. # Required. Destination on BigQuery. The output table stores the fields in asset proto as columns in BigQuery.
      &quot;dataset&quot;: &quot;A String&quot;, # Required. The BigQuery dataset in format &quot;projects/projectId/datasets/datasetId&quot;, to which the snapshot result should be exported. If this dataset does not exist, the export call returns an INVALID_ARGUMENT error.
      &quot;force&quot;: True or False, # If the destination table already exists and this flag is `TRUE`, the table will be overwritten by the contents of assets snapshot. If the flag is `FALSE` or unset and the destination table already exists, the export call returns an INVALID_ARGUMEMT error.
      &quot;partitionSpec&quot;: { # Specifications of BigQuery partitioned table as export destination. # [partition_spec] determines whether to export to partitioned table(s) and how to partition the data. If [partition_spec] is unset or [partition_spec.partition_key] is unset or `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to non-partitioned table(s). [force] will decide whether to overwrite existing table(s). If [partition_spec] is specified. First, the snapshot results will be written to partitioned table(s) with two additional timestamp columns, readTime and requestTime, one of which will be the partition key. Secondly, in the case when any destination table already exists, it will first try to update existing table&#x27;s schema as necessary by appending additional columns. Then, if [force] is `TRUE`, the corresponding partition will be overwritten by the snapshot results (data in different partitions will remain intact); if [force] is unset or `FALSE`, it will append the data. An error will be returned if the schema update or data appension fails.
        &quot;partitionKey&quot;: &quot;A String&quot;, # The partition key for BigQuery partitioned table.
      },
      &quot;separateTablesPerAssetType&quot;: True or False, # If this flag is `TRUE`, the snapshot results will be written to one or multiple tables, each of which contains results of one asset type. The [force] and [partition_spec] fields will apply to each of them. Field [table] will be concatenated with &quot;_&quot; and the asset type names (see https://cloud.google.com/asset-inventory/docs/supported-asset-types for supported asset types) to construct per-asset-type table names, in which all non-alphanumeric characters like &quot;.&quot; and &quot;/&quot; will be substituted by &quot;_&quot;. Example: if field [table] is &quot;mytable&quot; and snapshot results contain &quot;storage.googleapis.com/Bucket&quot; assets, the corresponding table name will be &quot;mytable_storage_googleapis_com_Bucket&quot;. If any of these tables does not exist, a new table with the concatenated name will be created. When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of each table will include RECORD-type columns mapped to the nested fields in the Asset.resource.data field of that asset type (up to the 15 nested level BigQuery supports (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The fields in &gt;15 nested levels will be stored in JSON format string as a child column of its parent RECORD column. If error occurs when exporting to any table, the whole export call will return an error but the export results that already succeed will persist. Example: if exporting to table_type_A succeeds when exporting to table_type_B fails during one export call, the results in table_type_A will persist and there will not be partial results persisting in a table.
      &quot;table&quot;: &quot;A String&quot;, # Required. The BigQuery table to which the snapshot result should be written. If this table does not exist, a new table with the given name will be created. In the case of scheduled export, the real destination is [table] appended with a suffix indicating the point-in-time of the snapshot. For example, suppose field [table] is &quot;mytable&quot;. When [separate_tables_per_asset_type] and is `FALSE`, the destination will be &quot;mytable_snapshot_20201122060000&quot;, and when [separate_tables_per_asset_type] and is `TRUE` and the [asset_types] contains &quot;storage.googleapis.com/Bucket&quot;, the destination will be &quot;mytable_snapshot_20201122060000_storage_googleapis_com_Bucket&quot;
    },
    &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
      &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
      &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
    },
  },
  &quot;contentType&quot;: &quot;A String&quot;, # Required. Asset content type. Currently only `RESOURCE`, `IAM_POLICY`, `ORG_POLICY`, `ACCESS_POLICY` are supported.
  &quot;gcsSetting&quot;: { # Setting for scheduled export to GCS. # Setting to configure scheduled export output to GCS. At least one of [gcs_setting] and [bigquery_setting] should be set.
    &quot;destination&quot;: { # A Cloud Storage location. # Required. Destination on Cloud Storage.
      &quot;uri&quot;: &quot;A String&quot;, # The uri of the Cloud Storage object. It&#x27;s the same uri that is used by gsutil. Example: &quot;gs://bucket_name/object_name&quot;. See [Viewing and Editing Object Metadata](https://cloud.google.com/storage/docs/viewing-editing-metadata) for more information. In the case of scheduled export, a path indiciating when the export happens will be added in between the bucket_name and object_name. For example: the real destination for an scheduled snapshot is: gs://bucket_name/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/file
      &quot;uriPrefix&quot;: &quot;A String&quot;, # The uri prefix of all generated Cloud Storage objects. Example: &quot;gs://bucket_name/object_name_prefix&quot;. Each object uri is in format: &quot;gs://bucket_name/object_name_prefix/{ASSET_TYPE}/{SHARD_NUMBER} and only contains assets for that type. starts from 0. Example: &quot;gs://bucket_name/object_name_prefix/compute.googleapis.com/Disk/0&quot; is the first shard of output objects containing all compute.googleapis.com/Disk assets. An INVALID_ARGUMENT error will be returned if file with the same name &quot;gs://bucket_name/object_name_prefix&quot; already exists. In the case of scheduled export, a path indiciating when the export happens will be appended. For example: the real destination for an scheduled snapshot is: gs://bucket_name/object_name_prefix/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/{ASSET_TYPE}/{SHARD_NUMBER}
    },
    &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
      &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
      &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
    },
  },
  &quot;name&quot;: &quot;A String&quot;, # The format will be: organizations/{ORGANIZATION_NUMBER}/inventorySettings/exportSettings/{exportSetting} or folders/{FOLDER_NUMBER}/inventorySettings/exportSettings/{exportSetting} or projects/{PROJECT_NUMBER}/inventorySettings/exportSettings/{exportSetting}
}</pre>
</div>

<div class="method">
    <code class="details" id="list">list(parent, pageSize=None, pageToken=None, x__xgafv=None)</code>
  <pre>Lists all export settings under a [InventorySettings].

Args:
  parent: string, Required. The parent [InventorySettings] whose export settings are to be listed. (required)
  pageSize: integer, The maximum number of export settings to return. The service may return fewer than this value. If unspecified, at most 100 export settings will be returned. The maximum value is 100, as currently only a maximum of 100 export settings can be created under each parent.
  pageToken: string, A page token, received from a previous `ListExportSettings` call. Provide this to retrieve the subsequent page. When paginating, all other parameters provided to `ListExportSettings` must match the call that provided the page token.
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    {
  &quot;exportSettings&quot;: [ # A list of export settings.
    { # Settings that configure CAI to export assets on given schedules. An [InventorySettings] might have multiple [ExportSetting], each with a unique identifier.
      &quot;assetTypes&quot;: [ # Asset types of resources that will be exported on schedule. For example: &quot;compute.googleapis.com/Disk&quot;. Regular expressions are also supported. For example: * &quot;compute.googleapis.com.*&quot; snapshots resources whose asset type starts with &quot;compute.googleapis.com&quot;. * &quot;.*Instance&quot; snapshots resources whose asset type ends with &quot;Instance&quot;. * &quot;.*Instance.*&quot; snapshots resources whose asset type contains &quot;Instance&quot;. See [RE2](https://github.com/google/re2/wiki/Syntax) for all supported regular expression syntax. If the regular expression does not match any supported asset type, an INVALID_ARGUMENT error will be returned. If specified, only matching assets will be returned. See [Introduction to Cloud Asset Inventory](https://cloud.google.com/asset-inventory/docs/overview) for all supported asset types.
        &quot;A String&quot;,
      ],
      &quot;bigquerySetting&quot;: { # Setting for scheduled export to BigQuery. # Setting to configure scheduled export output to BigQuery. At least one of [gcs_settings] and [bigquery_settings] should be set.
        &quot;destination&quot;: { # A BigQuery destination for exporting assets to. # Required. Destination on BigQuery. The output table stores the fields in asset proto as columns in BigQuery.
          &quot;dataset&quot;: &quot;A String&quot;, # Required. The BigQuery dataset in format &quot;projects/projectId/datasets/datasetId&quot;, to which the snapshot result should be exported. If this dataset does not exist, the export call returns an INVALID_ARGUMENT error.
          &quot;force&quot;: True or False, # If the destination table already exists and this flag is `TRUE`, the table will be overwritten by the contents of assets snapshot. If the flag is `FALSE` or unset and the destination table already exists, the export call returns an INVALID_ARGUMEMT error.
          &quot;partitionSpec&quot;: { # Specifications of BigQuery partitioned table as export destination. # [partition_spec] determines whether to export to partitioned table(s) and how to partition the data. If [partition_spec] is unset or [partition_spec.partition_key] is unset or `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to non-partitioned table(s). [force] will decide whether to overwrite existing table(s). If [partition_spec] is specified. First, the snapshot results will be written to partitioned table(s) with two additional timestamp columns, readTime and requestTime, one of which will be the partition key. Secondly, in the case when any destination table already exists, it will first try to update existing table&#x27;s schema as necessary by appending additional columns. Then, if [force] is `TRUE`, the corresponding partition will be overwritten by the snapshot results (data in different partitions will remain intact); if [force] is unset or `FALSE`, it will append the data. An error will be returned if the schema update or data appension fails.
            &quot;partitionKey&quot;: &quot;A String&quot;, # The partition key for BigQuery partitioned table.
          },
          &quot;separateTablesPerAssetType&quot;: True or False, # If this flag is `TRUE`, the snapshot results will be written to one or multiple tables, each of which contains results of one asset type. The [force] and [partition_spec] fields will apply to each of them. Field [table] will be concatenated with &quot;_&quot; and the asset type names (see https://cloud.google.com/asset-inventory/docs/supported-asset-types for supported asset types) to construct per-asset-type table names, in which all non-alphanumeric characters like &quot;.&quot; and &quot;/&quot; will be substituted by &quot;_&quot;. Example: if field [table] is &quot;mytable&quot; and snapshot results contain &quot;storage.googleapis.com/Bucket&quot; assets, the corresponding table name will be &quot;mytable_storage_googleapis_com_Bucket&quot;. If any of these tables does not exist, a new table with the concatenated name will be created. When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of each table will include RECORD-type columns mapped to the nested fields in the Asset.resource.data field of that asset type (up to the 15 nested level BigQuery supports (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The fields in &gt;15 nested levels will be stored in JSON format string as a child column of its parent RECORD column. If error occurs when exporting to any table, the whole export call will return an error but the export results that already succeed will persist. Example: if exporting to table_type_A succeeds when exporting to table_type_B fails during one export call, the results in table_type_A will persist and there will not be partial results persisting in a table.
          &quot;table&quot;: &quot;A String&quot;, # Required. The BigQuery table to which the snapshot result should be written. If this table does not exist, a new table with the given name will be created. In the case of scheduled export, the real destination is [table] appended with a suffix indicating the point-in-time of the snapshot. For example, suppose field [table] is &quot;mytable&quot;. When [separate_tables_per_asset_type] and is `FALSE`, the destination will be &quot;mytable_snapshot_20201122060000&quot;, and when [separate_tables_per_asset_type] and is `TRUE` and the [asset_types] contains &quot;storage.googleapis.com/Bucket&quot;, the destination will be &quot;mytable_snapshot_20201122060000_storage_googleapis_com_Bucket&quot;
        },
        &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
          &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
          &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
        },
      },
      &quot;contentType&quot;: &quot;A String&quot;, # Required. Asset content type. Currently only `RESOURCE`, `IAM_POLICY`, `ORG_POLICY`, `ACCESS_POLICY` are supported.
      &quot;gcsSetting&quot;: { # Setting for scheduled export to GCS. # Setting to configure scheduled export output to GCS. At least one of [gcs_setting] and [bigquery_setting] should be set.
        &quot;destination&quot;: { # A Cloud Storage location. # Required. Destination on Cloud Storage.
          &quot;uri&quot;: &quot;A String&quot;, # The uri of the Cloud Storage object. It&#x27;s the same uri that is used by gsutil. Example: &quot;gs://bucket_name/object_name&quot;. See [Viewing and Editing Object Metadata](https://cloud.google.com/storage/docs/viewing-editing-metadata) for more information. In the case of scheduled export, a path indiciating when the export happens will be added in between the bucket_name and object_name. For example: the real destination for an scheduled snapshot is: gs://bucket_name/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/file
          &quot;uriPrefix&quot;: &quot;A String&quot;, # The uri prefix of all generated Cloud Storage objects. Example: &quot;gs://bucket_name/object_name_prefix&quot;. Each object uri is in format: &quot;gs://bucket_name/object_name_prefix/{ASSET_TYPE}/{SHARD_NUMBER} and only contains assets for that type. starts from 0. Example: &quot;gs://bucket_name/object_name_prefix/compute.googleapis.com/Disk/0&quot; is the first shard of output objects containing all compute.googleapis.com/Disk assets. An INVALID_ARGUMENT error will be returned if file with the same name &quot;gs://bucket_name/object_name_prefix&quot; already exists. In the case of scheduled export, a path indiciating when the export happens will be appended. For example: the real destination for an scheduled snapshot is: gs://bucket_name/object_name_prefix/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/{ASSET_TYPE}/{SHARD_NUMBER}
        },
        &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
          &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
          &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
        },
      },
      &quot;name&quot;: &quot;A String&quot;, # The format will be: organizations/{ORGANIZATION_NUMBER}/inventorySettings/exportSettings/{exportSetting} or folders/{FOLDER_NUMBER}/inventorySettings/exportSettings/{exportSetting} or projects/{PROJECT_NUMBER}/inventorySettings/exportSettings/{exportSetting}
    },
  ],
  &quot;nextPageToken&quot;: &quot;A String&quot;, # Token to retrieve the next page of results. Set to empty if there are no remaining results.
}</pre>
</div>

<div class="method">
    <code class="details" id="list_next">list_next(previous_request, previous_response)</code>
  <pre>Retrieves the next page of results.

Args:
  previous_request: The request for the previous page. (required)
  previous_response: The response from the request for the previous page. (required)

Returns:
  A request object that you can call &#x27;execute()&#x27; on to request the next
  page. Returns None if there are no more items in the collection.
    </pre>
</div>

<div class="method">
    <code class="details" id="patch">patch(name, body=None, updateMask=None, x__xgafv=None)</code>
  <pre>Updates an export setting.

Args:
  name: string, The format will be: organizations/{ORGANIZATION_NUMBER}/inventorySettings/exportSettings/{exportSetting} or folders/{FOLDER_NUMBER}/inventorySettings/exportSettings/{exportSetting} or projects/{PROJECT_NUMBER}/inventorySettings/exportSettings/{exportSetting} (required)
  body: object, The request body.
    The object takes the form of:

{ # Settings that configure CAI to export assets on given schedules. An [InventorySettings] might have multiple [ExportSetting], each with a unique identifier.
  &quot;assetTypes&quot;: [ # Asset types of resources that will be exported on schedule. For example: &quot;compute.googleapis.com/Disk&quot;. Regular expressions are also supported. For example: * &quot;compute.googleapis.com.*&quot; snapshots resources whose asset type starts with &quot;compute.googleapis.com&quot;. * &quot;.*Instance&quot; snapshots resources whose asset type ends with &quot;Instance&quot;. * &quot;.*Instance.*&quot; snapshots resources whose asset type contains &quot;Instance&quot;. See [RE2](https://github.com/google/re2/wiki/Syntax) for all supported regular expression syntax. If the regular expression does not match any supported asset type, an INVALID_ARGUMENT error will be returned. If specified, only matching assets will be returned. See [Introduction to Cloud Asset Inventory](https://cloud.google.com/asset-inventory/docs/overview) for all supported asset types.
    &quot;A String&quot;,
  ],
  &quot;bigquerySetting&quot;: { # Setting for scheduled export to BigQuery. # Setting to configure scheduled export output to BigQuery. At least one of [gcs_settings] and [bigquery_settings] should be set.
    &quot;destination&quot;: { # A BigQuery destination for exporting assets to. # Required. Destination on BigQuery. The output table stores the fields in asset proto as columns in BigQuery.
      &quot;dataset&quot;: &quot;A String&quot;, # Required. The BigQuery dataset in format &quot;projects/projectId/datasets/datasetId&quot;, to which the snapshot result should be exported. If this dataset does not exist, the export call returns an INVALID_ARGUMENT error.
      &quot;force&quot;: True or False, # If the destination table already exists and this flag is `TRUE`, the table will be overwritten by the contents of assets snapshot. If the flag is `FALSE` or unset and the destination table already exists, the export call returns an INVALID_ARGUMEMT error.
      &quot;partitionSpec&quot;: { # Specifications of BigQuery partitioned table as export destination. # [partition_spec] determines whether to export to partitioned table(s) and how to partition the data. If [partition_spec] is unset or [partition_spec.partition_key] is unset or `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to non-partitioned table(s). [force] will decide whether to overwrite existing table(s). If [partition_spec] is specified. First, the snapshot results will be written to partitioned table(s) with two additional timestamp columns, readTime and requestTime, one of which will be the partition key. Secondly, in the case when any destination table already exists, it will first try to update existing table&#x27;s schema as necessary by appending additional columns. Then, if [force] is `TRUE`, the corresponding partition will be overwritten by the snapshot results (data in different partitions will remain intact); if [force] is unset or `FALSE`, it will append the data. An error will be returned if the schema update or data appension fails.
        &quot;partitionKey&quot;: &quot;A String&quot;, # The partition key for BigQuery partitioned table.
      },
      &quot;separateTablesPerAssetType&quot;: True or False, # If this flag is `TRUE`, the snapshot results will be written to one or multiple tables, each of which contains results of one asset type. The [force] and [partition_spec] fields will apply to each of them. Field [table] will be concatenated with &quot;_&quot; and the asset type names (see https://cloud.google.com/asset-inventory/docs/supported-asset-types for supported asset types) to construct per-asset-type table names, in which all non-alphanumeric characters like &quot;.&quot; and &quot;/&quot; will be substituted by &quot;_&quot;. Example: if field [table] is &quot;mytable&quot; and snapshot results contain &quot;storage.googleapis.com/Bucket&quot; assets, the corresponding table name will be &quot;mytable_storage_googleapis_com_Bucket&quot;. If any of these tables does not exist, a new table with the concatenated name will be created. When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of each table will include RECORD-type columns mapped to the nested fields in the Asset.resource.data field of that asset type (up to the 15 nested level BigQuery supports (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The fields in &gt;15 nested levels will be stored in JSON format string as a child column of its parent RECORD column. If error occurs when exporting to any table, the whole export call will return an error but the export results that already succeed will persist. Example: if exporting to table_type_A succeeds when exporting to table_type_B fails during one export call, the results in table_type_A will persist and there will not be partial results persisting in a table.
      &quot;table&quot;: &quot;A String&quot;, # Required. The BigQuery table to which the snapshot result should be written. If this table does not exist, a new table with the given name will be created. In the case of scheduled export, the real destination is [table] appended with a suffix indicating the point-in-time of the snapshot. For example, suppose field [table] is &quot;mytable&quot;. When [separate_tables_per_asset_type] and is `FALSE`, the destination will be &quot;mytable_snapshot_20201122060000&quot;, and when [separate_tables_per_asset_type] and is `TRUE` and the [asset_types] contains &quot;storage.googleapis.com/Bucket&quot;, the destination will be &quot;mytable_snapshot_20201122060000_storage_googleapis_com_Bucket&quot;
    },
    &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
      &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
      &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
    },
  },
  &quot;contentType&quot;: &quot;A String&quot;, # Required. Asset content type. Currently only `RESOURCE`, `IAM_POLICY`, `ORG_POLICY`, `ACCESS_POLICY` are supported.
  &quot;gcsSetting&quot;: { # Setting for scheduled export to GCS. # Setting to configure scheduled export output to GCS. At least one of [gcs_setting] and [bigquery_setting] should be set.
    &quot;destination&quot;: { # A Cloud Storage location. # Required. Destination on Cloud Storage.
      &quot;uri&quot;: &quot;A String&quot;, # The uri of the Cloud Storage object. It&#x27;s the same uri that is used by gsutil. Example: &quot;gs://bucket_name/object_name&quot;. See [Viewing and Editing Object Metadata](https://cloud.google.com/storage/docs/viewing-editing-metadata) for more information. In the case of scheduled export, a path indiciating when the export happens will be added in between the bucket_name and object_name. For example: the real destination for an scheduled snapshot is: gs://bucket_name/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/file
      &quot;uriPrefix&quot;: &quot;A String&quot;, # The uri prefix of all generated Cloud Storage objects. Example: &quot;gs://bucket_name/object_name_prefix&quot;. Each object uri is in format: &quot;gs://bucket_name/object_name_prefix/{ASSET_TYPE}/{SHARD_NUMBER} and only contains assets for that type. starts from 0. Example: &quot;gs://bucket_name/object_name_prefix/compute.googleapis.com/Disk/0&quot; is the first shard of output objects containing all compute.googleapis.com/Disk assets. An INVALID_ARGUMENT error will be returned if file with the same name &quot;gs://bucket_name/object_name_prefix&quot; already exists. In the case of scheduled export, a path indiciating when the export happens will be appended. For example: the real destination for an scheduled snapshot is: gs://bucket_name/object_name_prefix/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/{ASSET_TYPE}/{SHARD_NUMBER}
    },
    &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
      &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
      &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
    },
  },
  &quot;name&quot;: &quot;A String&quot;, # The format will be: organizations/{ORGANIZATION_NUMBER}/inventorySettings/exportSettings/{exportSetting} or folders/{FOLDER_NUMBER}/inventorySettings/exportSettings/{exportSetting} or projects/{PROJECT_NUMBER}/inventorySettings/exportSettings/{exportSetting}
}

  updateMask: string, Required. Only updates the `export_setting` fields indicated by this mask. The field mask must not be empty, and it must not contain fields that are immutable or only set by the server.
  x__xgafv: string, V1 error format.
    Allowed values
      1 - v1 error format
      2 - v2 error format

Returns:
  An object of the form:

    { # Settings that configure CAI to export assets on given schedules. An [InventorySettings] might have multiple [ExportSetting], each with a unique identifier.
  &quot;assetTypes&quot;: [ # Asset types of resources that will be exported on schedule. For example: &quot;compute.googleapis.com/Disk&quot;. Regular expressions are also supported. For example: * &quot;compute.googleapis.com.*&quot; snapshots resources whose asset type starts with &quot;compute.googleapis.com&quot;. * &quot;.*Instance&quot; snapshots resources whose asset type ends with &quot;Instance&quot;. * &quot;.*Instance.*&quot; snapshots resources whose asset type contains &quot;Instance&quot;. See [RE2](https://github.com/google/re2/wiki/Syntax) for all supported regular expression syntax. If the regular expression does not match any supported asset type, an INVALID_ARGUMENT error will be returned. If specified, only matching assets will be returned. See [Introduction to Cloud Asset Inventory](https://cloud.google.com/asset-inventory/docs/overview) for all supported asset types.
    &quot;A String&quot;,
  ],
  &quot;bigquerySetting&quot;: { # Setting for scheduled export to BigQuery. # Setting to configure scheduled export output to BigQuery. At least one of [gcs_settings] and [bigquery_settings] should be set.
    &quot;destination&quot;: { # A BigQuery destination for exporting assets to. # Required. Destination on BigQuery. The output table stores the fields in asset proto as columns in BigQuery.
      &quot;dataset&quot;: &quot;A String&quot;, # Required. The BigQuery dataset in format &quot;projects/projectId/datasets/datasetId&quot;, to which the snapshot result should be exported. If this dataset does not exist, the export call returns an INVALID_ARGUMENT error.
      &quot;force&quot;: True or False, # If the destination table already exists and this flag is `TRUE`, the table will be overwritten by the contents of assets snapshot. If the flag is `FALSE` or unset and the destination table already exists, the export call returns an INVALID_ARGUMEMT error.
      &quot;partitionSpec&quot;: { # Specifications of BigQuery partitioned table as export destination. # [partition_spec] determines whether to export to partitioned table(s) and how to partition the data. If [partition_spec] is unset or [partition_spec.partition_key] is unset or `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to non-partitioned table(s). [force] will decide whether to overwrite existing table(s). If [partition_spec] is specified. First, the snapshot results will be written to partitioned table(s) with two additional timestamp columns, readTime and requestTime, one of which will be the partition key. Secondly, in the case when any destination table already exists, it will first try to update existing table&#x27;s schema as necessary by appending additional columns. Then, if [force] is `TRUE`, the corresponding partition will be overwritten by the snapshot results (data in different partitions will remain intact); if [force] is unset or `FALSE`, it will append the data. An error will be returned if the schema update or data appension fails.
        &quot;partitionKey&quot;: &quot;A String&quot;, # The partition key for BigQuery partitioned table.
      },
      &quot;separateTablesPerAssetType&quot;: True or False, # If this flag is `TRUE`, the snapshot results will be written to one or multiple tables, each of which contains results of one asset type. The [force] and [partition_spec] fields will apply to each of them. Field [table] will be concatenated with &quot;_&quot; and the asset type names (see https://cloud.google.com/asset-inventory/docs/supported-asset-types for supported asset types) to construct per-asset-type table names, in which all non-alphanumeric characters like &quot;.&quot; and &quot;/&quot; will be substituted by &quot;_&quot;. Example: if field [table] is &quot;mytable&quot; and snapshot results contain &quot;storage.googleapis.com/Bucket&quot; assets, the corresponding table name will be &quot;mytable_storage_googleapis_com_Bucket&quot;. If any of these tables does not exist, a new table with the concatenated name will be created. When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of each table will include RECORD-type columns mapped to the nested fields in the Asset.resource.data field of that asset type (up to the 15 nested level BigQuery supports (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The fields in &gt;15 nested levels will be stored in JSON format string as a child column of its parent RECORD column. If error occurs when exporting to any table, the whole export call will return an error but the export results that already succeed will persist. Example: if exporting to table_type_A succeeds when exporting to table_type_B fails during one export call, the results in table_type_A will persist and there will not be partial results persisting in a table.
      &quot;table&quot;: &quot;A String&quot;, # Required. The BigQuery table to which the snapshot result should be written. If this table does not exist, a new table with the given name will be created. In the case of scheduled export, the real destination is [table] appended with a suffix indicating the point-in-time of the snapshot. For example, suppose field [table] is &quot;mytable&quot;. When [separate_tables_per_asset_type] and is `FALSE`, the destination will be &quot;mytable_snapshot_20201122060000&quot;, and when [separate_tables_per_asset_type] and is `TRUE` and the [asset_types] contains &quot;storage.googleapis.com/Bucket&quot;, the destination will be &quot;mytable_snapshot_20201122060000_storage_googleapis_com_Bucket&quot;
    },
    &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
      &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
      &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
    },
  },
  &quot;contentType&quot;: &quot;A String&quot;, # Required. Asset content type. Currently only `RESOURCE`, `IAM_POLICY`, `ORG_POLICY`, `ACCESS_POLICY` are supported.
  &quot;gcsSetting&quot;: { # Setting for scheduled export to GCS. # Setting to configure scheduled export output to GCS. At least one of [gcs_setting] and [bigquery_setting] should be set.
    &quot;destination&quot;: { # A Cloud Storage location. # Required. Destination on Cloud Storage.
      &quot;uri&quot;: &quot;A String&quot;, # The uri of the Cloud Storage object. It&#x27;s the same uri that is used by gsutil. Example: &quot;gs://bucket_name/object_name&quot;. See [Viewing and Editing Object Metadata](https://cloud.google.com/storage/docs/viewing-editing-metadata) for more information. In the case of scheduled export, a path indiciating when the export happens will be added in between the bucket_name and object_name. For example: the real destination for an scheduled snapshot is: gs://bucket_name/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/file
      &quot;uriPrefix&quot;: &quot;A String&quot;, # The uri prefix of all generated Cloud Storage objects. Example: &quot;gs://bucket_name/object_name_prefix&quot;. Each object uri is in format: &quot;gs://bucket_name/object_name_prefix/{ASSET_TYPE}/{SHARD_NUMBER} and only contains assets for that type. starts from 0. Example: &quot;gs://bucket_name/object_name_prefix/compute.googleapis.com/Disk/0&quot; is the first shard of output objects containing all compute.googleapis.com/Disk assets. An INVALID_ARGUMENT error will be returned if file with the same name &quot;gs://bucket_name/object_name_prefix&quot; already exists. In the case of scheduled export, a path indiciating when the export happens will be appended. For example: the real destination for an scheduled snapshot is: gs://bucket_name/object_name_prefix/{YYYY}/{MM}/{DD}/snapshot/{HHMMSS}/{ASSET_TYPE}/{SHARD_NUMBER}
    },
    &quot;snapshotSchedule&quot;: { # Schedule for scheduled export. This is used to configure scheduled export for both GCS and BigQuery. # Schedule for scheduled export of asset snapshot.
      &quot;crontab&quot;: &quot;A String&quot;, # Required. Cron-tab formatted schedule by which the job will execute Format: minute, hour, day of month, month, day of week e.g. 0 0 * * WED = every Wednesday More examples: https://crontab.guru/examples.html
      &quot;timeZone&quot;: &quot;A String&quot;, # Specifies the time zone to be used in interpreting [crontab]. The value of this field must be a time zone name from the [tz database](https://en.wikipedia.org/wiki/Tz_database). Note that some time zones include a provision for daylight savings time. The rules for daylight saving time are determined by the chosen tz. For UTC use the string &quot;utc&quot;. If a time zone is not specified, the default will be in UTC (also known as GMT).
    },
  },
  &quot;name&quot;: &quot;A String&quot;, # The format will be: organizations/{ORGANIZATION_NUMBER}/inventorySettings/exportSettings/{exportSetting} or folders/{FOLDER_NUMBER}/inventorySettings/exportSettings/{exportSetting} or projects/{PROJECT_NUMBER}/inventorySettings/exportSettings/{exportSetting}
}</pre>
</div>

</body></html>